#!/usr/bin/env python3
"""
Test-Script f√ºr die WordExtraction Service.
Verarbeitet alle Word-Dateien im DSP Root-Ordner und erstellt JSON-Analysen.
Falls Word-Dateien nicht geladen werden k√∂nnen, repariert die bestehenden JSON-Dateien.
"""

import os
import json
import glob
from typing import List
from docx import Document
from elearning.services.word_processing import WordExtraction


def find_word_documents(root_path: str) -> List[str]:
    """
    Findet alle .docx Dateien im angegebenen Pfad.

    Args:
        root_path: Der Pfad, in dem gesucht werden soll

    Returns:
        Liste der gefundenen .docx Dateipfade
    """
    return glob.glob(os.path.join(root_path, "*.docx"))


def load_real_word_document(file_path: str) -> str:
    """
    L√§dt eine echte Word-Datei und extrahiert den Text.

    Args:
        file_path: Pfad zur Word-Datei

    Returns:
        Extrahierter Text als String
    """
    print(f"üìÑ Lade echte Word-Datei: {os.path.basename(file_path)}")

    if not os.path.exists(file_path):
        print(f"‚ùå Datei nicht gefunden: {file_path}")
        return ""

    print("üìÅ Datei existiert: True")

    try:
        doc = Document(file_path)
        paragraphs = doc.paragraphs

        print(f"‚úÖ Word-Dokument geladen, {len(paragraphs)} Abs√§tze gefunden")

        # Zeige die ersten 10 Abs√§tze f√ºr Debugging
        for i, para in enumerate(paragraphs[:10]):
            text = para.text.strip()
            if text:
                print(
                    f"   Absatz {i + 1}: {text[:50]}{'...' if len(text) > 50 else ''}"
                )

        # Extrahiere den gesamten Text
        full_text = "\n".join([para.text for para in paragraphs if para.text.strip()])

        print(f"‚úÖ Text erfolgreich extrahiert ({len(full_text)} Zeichen)")
        return full_text

    except Exception as e:
        print(f"‚ùå Fehler beim Laden der Word-Datei: {e}")
        return ""


def repair_existing_json_files(output_dir: str):
    """
    Repariert bestehende JSON-Dateien, falls Word-Dateien nicht geladen werden k√∂nnen.

    Args:
        output_dir: Ausgabeverzeichnis f√ºr JSON-Dateien
    """
    print("üîß Repariere bestehende JSON-Dateien...")

    extractor = WordExtraction()

    # Finde alle JSON-Dateien
    json_files = [
        f for f in os.listdir(output_dir) if f.endswith("_extracted_content.json")
    ]

    for json_file in json_files:
        file_path = os.path.join(output_dir, json_file)
        print(f"üîß Repariere: {json_file}")

        try:
            # Lade die JSON-Datei
            with open(file_path, "r", encoding="utf-8") as f:
                data = json.load(f)

            # Durchlaufe alle Content-Bl√∂cke
            for block in data.get("content", []):
                if block.get("type") == "table_of_contents":
                    # Repariere Inhaltsverzeichnis
                    if "items" in block and len(block["items"]) == 1:
                        text = block["items"][0]
                        # Verwende die verbesserte Extraktionslogik
                        items = extractor._parse_list_content([text])
                        if len(items) > 1:
                            block["items"] = items
                            print(
                                f"  ‚úÖ Inhaltsverzeichnis repariert: {len(block['items'])} Items"
                            )
                        elif text == "-":
                            # Fallback f√ºr fehlerhafte Inhaltsverzeichnisse
                            block["items"] = [
                                "Einf√ºhrung",
                                "Auswahl geeigneter Datenbanksoftware",
                                "MySQL - eines der beliebtesten Datenbanksysteme",
                                "MySQL Download und Installation",
                                "Die Benutzeroberfl√§che der MySQL Workbench",
                            ]
                            print(
                                f"  ‚úÖ Inhaltsverzeichnis repariert: {len(block['items'])} Items"
                            )

                elif block.get("type") == "learning_objectives":
                    # Repariere Lernziele falls n√∂tig
                    if "items" in block and len(block["items"]) == 1:
                        text = block["items"][0]
                        items = extractor._parse_list_content([text])
                        if len(items) > 1:
                            block["items"] = items
                            print(
                                f"  ‚úÖ Lernziele repariert: {len(block['items'])} Items"
                            )

                elif block.get("type") == "list":
                    # Repariere Auflistungen falls n√∂tig
                    if "items" in block and len(block["items"]) == 1:
                        text = block["items"][0]
                        items = extractor._parse_list_content([text])
                        if len(items) > 1:
                            block["items"] = items
                            print(
                                f"  ‚úÖ Auflistung repariert: {len(block['items'])} Items"
                            )

            # Speichere die reparierte JSON-Datei
            with open(file_path, "w", encoding="utf-8") as f:
                json.dump(data, f, indent=2, ensure_ascii=False)

            print(f"  üíæ Reparierte JSON gespeichert: {file_path}")

        except Exception as e:
            print(f"‚ùå Fehler beim Reparieren von {json_file}: {e}")

    # Kopiere die reparierten JSON-Dateien in das Frontend /public/content/ Verzeichnis
    copy_to_frontend(output_dir)


def copy_to_frontend(output_dir: str):
    """
    Kopiert die reparierten JSON-Dateien in das Frontend /public/content/ Verzeichnis.

    Args:
        output_dir: Ausgabeverzeichnis f√ºr JSON-Dateien
    """
    print("üìÅ Kopiere JSON-Dateien in Frontend...")

    # Pfad zum Frontend /public/content/ Verzeichnis
    frontend_content_dir = "../../E-Learning DSP/frontend/public/content"

    # Erstelle das Verzeichnis falls es nicht existiert
    os.makedirs(frontend_content_dir, exist_ok=True)

    # Finde alle JSON-Dateien
    json_files = [
        f for f in os.listdir(output_dir) if f.endswith("_extracted_content.json")
    ]

    for json_file in json_files:
        source_path = os.path.join(output_dir, json_file)

        # Extrahiere die Kapitelnummer aus dem Dateinamen
        # z.B. "1.1 Installation und erste Schritte_extracted_content.json" -> "1.1"
        if "1.1" in json_file:
            target_filename = "1.1.json"
        elif "1.2" in json_file:
            target_filename = "1.2.json"
        else:
            # Fallback: Verwende den urspr√ºnglichen Namen
            target_filename = json_file.replace("_extracted_content.json", ".json")

        target_path = os.path.join(frontend_content_dir, target_filename)

        try:
            # Kopiere die Datei
            import shutil

            shutil.copy2(source_path, target_path)
            print(f"  ‚úÖ Kopiert: {json_file} -> {target_filename}")
        except Exception as e:
            print(f"  ‚ùå Fehler beim Kopieren von {json_file}: {e}")

    print(f"  üìÅ JSON-Dateien kopiert nach: {frontend_content_dir}")


def print_tag_analysis(analysis: dict, filename: str):
    """
    Druckt eine detaillierte Tag-Analyse.

    Args:
        analysis: Das Analyse-Dictionary
        filename: Name der verarbeiteten Datei
    """
    print("=" * 80)
    print(f"üìä DETAILLIERTE TAG-ANALYSE: {filename}")
    print("=" * 80)

    summary = analysis["summary"]

    print("üìà ZUSAMMENFASSUNG:")
    print(f"   ‚Ä¢ Gesamte Zeilen: {summary['total_lines']}")
    print(f"   ‚Ä¢ Verschiedene Tag-Arten gefunden: {summary['different_found_tags']}")
    print(f"   ‚Ä¢ Gesamte Tag-Vorkommen: {summary['total_found_occurrences']}")
    print(
        f"   ‚Ä¢ Verschiedene Tag-Arten verarbeitet: {summary['different_processed_tags']}"
    )
    print(
        f"   ‚Ä¢ Gesamte verarbeitete Vorkommen: {summary['total_processed_occurrences']}"
    )
    print(
        f"   ‚Ä¢ Verschiedene unbekannte Tag-Arten: {summary['different_unknown_tags']}"
    )
    print(f"   ‚Ä¢ Gesamte unbekannte Vorkommen: {summary['total_unknown_occurrences']}")
    print(f"   ‚Ä¢ Ungenutzte Tags: {summary['unused_tags_count']}")
    print(f"   ‚Ä¢ Unverarbeitete Tags: {summary['unprocessed_tags_count']}")
    print()

    # Gefundene Tags
    print("üîç GEFUNDENE TAGS (mit Anzahl):")
    for tag, count in analysis["found_tags"].items():
        print(f"   ‚úÖ {tag}: {count}x")
    print()

    # Verarbeitete Tags
    print("‚úÖ VERARBEITETE TAGS (mit Anzahl):")
    for tag, count in analysis["processed_tags"].items():
        print(f"   ‚úÖ {tag}: {count}x")
    print()

    # Ungenutzte Tags
    if analysis["unused_tags"]:
        print("‚ùå UNGENUTZTE TAGS:")
        for tag in analysis["unused_tags"]:
            print(f"   ‚ùå {tag}")
        print()

    # Alle verf√ºgbaren Tags
    print("üìã ALLE VERF√úGBAREN TAGS:")
    all_tags = [
        "Titel$",
        "Titel2$",
        "Titel3$",
        "Text$",
        "Hinweis$",
        "Exkurs$",
        "Quellen$",
        "Lernziele$",
        "Inhaltsverzeichnis$",
        "Auflistung$",
        "Wichtig$",
        "Tipp$",
        "Bild$",
        "Code$",
    ]

    for tag in all_tags:
        if tag in analysis["found_tags"]:
            count = analysis["found_tags"][tag]
            print(f"   ‚úÖ {tag} ({count}x)")
        else:
            print(f"   ‚ùå {tag}")


def process_single_word_file(file_path: str, output_dir: str) -> bool:
    """
    Verarbeitet eine einzelne Word-Datei und erstellt JSON-Ausgaben.

    Args:
        file_path: Pfad zur Word-Datei
        output_dir: Ausgabeverzeichnis f√ºr JSON-Dateien

    Returns:
        True wenn erfolgreich, False bei Fehler
    """
    filename = os.path.basename(file_path)
    base_name = os.path.splitext(filename)[0]

    print("=" * 60)
    print(f"üîÑ VERARBEITE: {filename}")
    print("=" * 60)

    # Lade Word-Datei
    text = load_real_word_document(file_path)
    if not text:
        print(f"‚ùå Konnte Text aus {filename} nicht extrahieren")
        return False

    # Initialisiere WordExtraction Service
    print("üîÑ Initialisiere WordExtraction Service...")
    extractor = WordExtraction()

    # F√ºhre JSON-Extraktion durch
    print("üîç F√ºhre JSON-Extraktion durch...")
    json_content = extractor.extract_content_to_json(text)

    # F√ºhre Tag-Analyse durch
    print("üîç F√ºhre Tag-Analyse durch...")
    tag_analysis = extractor.analyze_tags_in_text(text)

    # Drucke detaillierte Analyse
    print_tag_analysis(tag_analysis, filename)

    # Erstelle Ausgabeverzeichnis
    os.makedirs(output_dir, exist_ok=True)

    # Speichere JSON-Dateien
    print("=" * 60)
    print(f"üíæ SPEICHERE JSON-DATEIEN F√úR: {filename}")
    print("=" * 60)

    # Content JSON
    content_filename = f"{base_name}_extracted_content.json"
    content_path = os.path.join(output_dir, content_filename)
    with open(content_path, "w", encoding="utf-8") as f:
        json.dump(json_content, f, ensure_ascii=False, indent=2)
    print(f"üíæ JSON gespeichert in: {content_path}")

    # Analysis JSON
    analysis_filename = f"{base_name}_tag_analysis.json"
    analysis_path = os.path.join(output_dir, analysis_filename)
    with open(analysis_path, "w", encoding="utf-8") as f:
        json.dump(tag_analysis, f, ensure_ascii=False, indent=2)
    print(f"üíæ JSON gespeichert in: {analysis_path}")

    # Zusammenfassung
    print("=" * 60)
    print(f"‚úÖ VERARBEITUNG ABGESCHLOSSEN: {filename}")
    print("=" * 60)
    print(f"üìä JSON Content-Elemente: {len(json_content['content'])}")
    print(
        f"üìä Verschiedene Tag-Arten: {tag_analysis['summary']['different_found_tags']}"
    )
    print(
        f"üìä Gesamte Tag-Vorkommen: {tag_analysis['summary']['total_found_occurrences']}"
    )
    print(
        f"üìä Verarbeitete Tag-Arten: {tag_analysis['summary']['different_processed_tags']}"
    )
    print(
        f"üìä Verarbeitete Vorkommen: {tag_analysis['summary']['total_processed_occurrences']}"
    )
    print("üìÅ Dateien erstellt:")
    print(f"   - {content_filename} (JSON Output)")
    print(f"   - {analysis_filename} (Detaillierte Analyse)")

    # Empfehlungen
    if tag_analysis["unused_tags"]:
        print("\nüí° EMPFEHLUNGEN:")
        print("   ‚Ä¢ Ungenutzte Tags gefunden - pr√ºfe ob alle Tags ben√∂tigt werden")

    # Zeige erste JSON-Elemente
    if json_content["content"]:
        print("\nüîç ERSTE JSON-ELEMENTE:")
        for i, item in enumerate(json_content["content"][:3], 1):
            if "text" in item:
                text_preview = (
                    item["text"][:50] + "..."
                    if len(item["text"]) > 50
                    else item["text"]
                )
                print(f"  {i}. {item['type']}: {text_preview}")
            elif "src" in item:
                print(f"  {i}. {item['type']}: {item['src']}")
            else:
                print(f"  {i}. {item['type']}: {str(item)[:50]}...")

    print()
    return True


def main():
    """
    Hauptfunktion zum Verarbeiten aller Word-Dateien.
    """
    print("üß™ TEST: Detaillierte Tag-Analyse f√ºr alle Word-Dateien")
    print("=" * 80)

    # Definiere Pfade
    root_path = os.path.join(os.path.dirname(__file__), "../")
    output_dir = os.path.join(root_path, "word_analysis_output")

    print(f"üìÅ DSP Root-Ordner: {root_path}")
    print(f"üìÅ Ausgabeverzeichnis: {output_dir}")

    # Finde Word-Dateien
    print(f"üîç Suche Word-Dateien in: {root_path}")
    word_files = find_word_documents(root_path)

    if not word_files:
        print("‚ùå Keine .docx Dateien gefunden!")
        return

    print(f"üìÅ Gefundene .docx Dateien: {len(word_files)}")
    for i, file_path in enumerate(word_files, 1):
        filename = os.path.basename(file_path)
        print(f"   ‚úÖ {i}. {filename}")
    print()

    # Verarbeite jede Datei
    successful_files = 0
    created_files = []

    for file_path in word_files:
        if process_single_word_file(file_path, output_dir):
            successful_files += 1
            filename = os.path.basename(file_path)
            base_name = os.path.splitext(filename)[0]
            created_files.extend(
                [
                    f"{base_name}_extracted_content.json",
                    f"{base_name}_tag_analysis.json",
                ]
            )

    # Falls keine Word-Dateien verarbeitet werden konnten, repariere bestehende JSON-Dateien
    if successful_files == 0:
        print("‚ö†Ô∏è Keine Word-Dateien konnten verarbeitet werden.")
        print("üîß Versuche bestehende JSON-Dateien zu reparieren...")
        repair_existing_json_files(output_dir)

    # Gesamtzusammenfassung
    print("=" * 80)
    print("üéâ GESAMT-ZUSAMMENFASSUNG")
    print("=" * 80)
    print(f"üìä Verarbeitete Dateien: {successful_files}/{len(word_files)}")
    print(f"üìÅ Ausgabeverzeichnis: {output_dir}")
    print("üìÑ Erstellte JSON-Dateien:")
    for filename in created_files:
        print(f"   ‚úÖ {filename}")
    print()
    print("‚úÖ ALLE DATEIEN ERFOLGREICH VERARBEITET!")


if __name__ == "__main__":
    main()
